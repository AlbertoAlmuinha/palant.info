---
author: Wladimir Palant
id: "000001"
---

<p>As outlined in reply to the comments above, a caching hit with a <span class="caps">CDN</span> isn&#8217;t terribly more likely than a caching hit for a library you host yourself. That&#8217;s because there is a number of different <span class="caps">CDN</span>s and lots of different library versions with each website relying on another one. And cache sizes aren&#8217;t unlimited, even with a lengthy expiration interval some files are bound to be evicted from cache. And even when the library is cached, it only saves you the <span class="caps">TCP</span> connection and <span class="caps">DNS</span> lookup you normally wouldn&#8217;t have had in the first place.</p>

	<p>Your points in the blog post are:</p>

	<ul>
		<li>Bandwidth: From what I can tell, most traffic tends to be generated by images and custom code, not standard libraries. So I would consider the scenario where outsourcing downloads of standard libraries makes a cost difference an edge case.</li>
		<li>Faster downloads due to different domain names: fortunately, this trick is increasingly losing its importance. All browsers increased their connection limits by now, and this technique is even considered counter-productive with <span class="caps">SPDY</span>.</li>
		<li><span class="caps">CDN</span> uptime: Even if <span class="caps">CDN</span> uptime is really 99.999%, that&#8217;s 0.001% of additional downtime for your website.</li>
		<li>Less data is being sent over the wire due to less headers/better compression: As long as all of the request still fits into one Ethernet frame (typical scenario, even with many cookie headers), there should no performance difference by making that frame smaller. Requiring one more <span class="caps">TCP</span> (and <span class="caps">SSL</span>?) handshake in order to connect to the <span class="caps">CDN</span> outweighs this advantage by far. As to better compression, the compression of jQuery 2.1.0 minified on code.jquery.org is actually very bad for some reason (34151 bytes from <span class="caps">CDN</span> whereas regular gzip command produces 29344 bytes). While cdnjs doesn&#8217;t fail quite as badly, it has a 29726 bytes download which is still too large. Only Google <span class="caps">CDN</span> manages to do better than my naive approach, it has a download that is 43 bytes smaller. Yes, totally worth it :-)</li>
		<li>Cross-site caching: see above.</li>
	</ul>

	<p>Your argumentation on security essentially boils down to: &#8220;everybody is doing it, so I&#8217;ll do it as well.&#8221; Sure, in case of Google <span class="caps">CDN</span> the risk is low. But still, Google <span class="caps">CDN</span> is a very interesting target for hackers (and governments), exactly because it is used by so many websites. If eventually your website turns out compromised through Google <span class="caps">CDN</span>, will seeing other websites suffer the same fate give you comfort? And wouldn&#8217;t it be better to avoid an unnecessary risk here in the first place?</p>